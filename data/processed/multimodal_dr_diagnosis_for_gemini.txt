
Hello Gemini. I need your expert help with my Python project for a multi-modal medical diagnosis system.

Your Role:
Act as an expert AI engineer and Python developer with deep specialization in multi-modal systems, computer vision, natural language processing (NLP), and the implementation of Retrieval-Augmented Generation (RAG) pipelines using LangChain and FastAPI.

Project Context:
The code I'm providing is a complete system for **Multi-Modal Diabetic Retinopathy (DR) Diagnosis**. It's designed to simulate a clinical diagnostic workflow by integrating computer vision models with a Large Language Model (LLM) powered by a private knowledge base.

The project follows a sophisticated, decoupled architecture:
1.  **Configuration (`settings.py`):** Centralized Pydantic settings manage all model paths and parameters, allowing easy configuration via a `.env` file.
2.  **Vision Processing (`vision_processors.py`):** This module is responsible for all visual tasks. It contains:
    *   A fine-tuned **ResNet50** model for DR grading (classifying the severity).
    *   A Visual Language Model (**Qwen-VL**) to generate descriptive text about key lesions from the fundus image.
3.  **LLM Loading (`llm_loader.py`):** Loads a fine-tuned Large Language Model (**R1-7B with LoRA**) and wraps it into a standard LangChain-compatible component for seamless integration.
4.  **RAG Pipeline (`rag_chain_builder.py`):** This is the core of the NLP logic. It uses **LangChain Expression Language (LCEL)** to build a RAG chain that:
    *   Loads private medical guidelines from the `knowledge_base/` directory.
    *   Creates a vector store using FAISS for efficient retrieval.
    *   Defines a sophisticated prompt template that guides the LLM to perform Chain-of-Thought (CoT) reasoning.
5.  **API Service (`main.py`):** A **FastAPI** application serves as the central controller. It exposes a single `/diagnose` endpoint that:
    *   Receives an uploaded fundus image.
    *   Orchestrates the calls to the vision processing modules (ResNet50 and Qwen-VL).
    *   Invokes the RAG chain with the results from the vision models.
    *   Returns a structured, traceable, and clinically relevant diagnostic report in JSON format.
6.  **Client Script (`run_diagnosis.py`):** A command-line tool to easily test the entire system by sending an image to the FastAPI server and printing the final report.

Your Task:

1.  **Analyze and Understand:** Carefully read and fully comprehend the entire Python codebase provided below. The code is split into multiple files, and you must understand how they interact. The main entry point for the service is `main.py`, and for testing is `run_diagnosis.py`.

2.  **Wait for Instructions:** After you have fully processed all the code, simply respond with: "I have analyzed the complete multi-modal DR diagnosis system. I understand the workflow, from visual analysis to the RAG-based generation of traceable diagnostic reports. I am ready to assist. What is your question?"

3.  **Assist Me:** Once you've given the confirmation message, I will ask you questions. You should then help me with tasks such as:
    *   Debugging specific errors in any part of the pipeline.
    *   Explaining complex parts of the code (e.g., the LCEL chain construction in `rag_chain_builder.py`).
    *   Suggesting code improvements for better performance, async handling, or modularity.
    *   Refactoring the code to follow different design patterns.
    *   Adding new features, such as caching mechanisms for the RAG retriever or integrating different VLM models.

Code Structure:
The complete source code is provided below. Each file is clearly delimited by `--- START OF FILE: [filepath] ---` and `--- END OF FILE: [filepath] ---` markers.


**Aggregated Files:**
- `llm_loader.py`
- `main.py`
- `medical-o1-reasoning-SFT/convert.py`
- `rag_chain_builder.py`
- `run_diagnosis.py`
- `settings.py`
- `vision_processors.py`

================================================================================
--- START OF AGGREGATED CODE ---
================================================================================

--- START OF FILE: llm_loader.py ---

# llm_loader.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from peft import PeftModel
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_core.language_models.chat_models import BaseChatModel

from settings import settings
import logging

logger = logging.getLogger(__name__)

def load_r1_7b_llm_as_langchain_component() -> BaseChatModel:
    """加载微调后的R1-7B模型，并将其封装为LangChain可用的组件。"""
    try:
        tokenizer = AutoTokenizer.from_pretrained(settings.R1_7B_MODEL_PATH, trust_remote_code=True)
        
        # 加载基础模型
        base_model = AutoModelForCausalLM.from_pretrained(
            settings.R1_7B_MODEL_PATH,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True
        )

        # 检查并应用LoRA适配器
        try:
            model = PeftModel.from_pretrained(base_model, settings.R1_7B_MODEL_PATH)
            logger.info(f"成功加载R1-7B模型并应用LoRA适配器: {settings.R1_7B_MODEL_PATH}")
        except ValueError:
            model = base_model
            logger.info(f"未找到LoRA适配器，加载R1-7B基础模型: {settings.R1_7B_MODEL_PATH}")

        # 创建Hugging Face Pipeline
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=1024,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.15
        )
        
        # 封装为LangChain LLM组件
        llm = HuggingFacePipeline(pipeline=pipe)
        return llm
    except Exception as e:
        logger.error(f"加载R1-7B模型失败: {e}")
        raise

--- END OF FILE: llm_loader.py ---


--- START OF FILE: main.py ---

# main.py
import uvicorn
import io
import json
import uuid
from datetime import datetime
from PIL import Image
from fastapi import FastAPI, File, UploadFile, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional

from settings import settings
from vision_processors import DRGradingModule, QwenVLModule
from llm_loader import load_r1_7b_llm_as_langchain_component
from rag_chain_builder import get_retriever, create_rag_chain
import logging

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- FastAPI应用和数据模型 ---
app = FastAPI(title="多模态DR智能诊断系统 V2 (LangChain集成版)")

class DiagnosisResponse(BaseModel):
    trace_id: str
    dr_grade: int
    dr_grade_desc: str
    confidence: float
    lesion_description: str
    structured_report: dict

# --- 全局资源加载 ---
# 使用全局变量来持有加载好的模型和链，避免重复加载
dr_grader = None
lesion_describer = None
rag_chain = None

@app.on_event("startup")
def startup_event():
    """在应用启动时加载所有模型和RAG链"""
    global dr_grader, lesion_describer, rag_chain
    logger.info("开始加载系统资源...")
    dr_grader = DRGradingModule(settings.RESNET_MODEL_PATH)
    lesion_describer = QwenVLModule(settings.QWEN_VL_MODEL_PATH)
    llm = load_r1_7b_llm_as_langchain_component()
    retriever = get_retriever()
    rag_chain = create_rag_chain(llm, retriever)
    logger.info("系统资源加载完成，应用已就绪。")

@app.post("/diagnose", response_model=DiagnosisResponse, summary="一键诊断DR图像")
async def diagnose(file: UploadFile = File(..., description="上传DR眼底图像")):
    trace_id = str(uuid.uuid4())
    logger.info(f"[{trace_id}] 收到新的诊断请求: {file.filename}")
    
    # 读取和验证图像
    try:
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert("RGB")
    except Exception as e:
        logger.error(f"[{trace_id}] 图像读取或验证失败: {e}")
        raise HTTPException(status_code=400, detail=f"无效的图像文件: {e}")

    # 1. 视觉处理
    logger.info(f"[{trace_id}] 正在进行DR分级...")
    dr_grade, confidence = dr_grader.predict(image)
    dr_grade_desc = settings.DR_GRADES.get(dr_grade, "未知等级")
    
    logger.info(f"[{trace_id}] 正在生成病灶描述...")
    lesion_description = lesion_describer.generate_description(image, dr_grade_desc)
    
    # 2. 调用RAG链生成结构化报告
    logger.info(f"[{trace_id}] 正在调用RAG链...")
    try:
        rag_input = {
            "dr_grade_desc": dr_grade_desc,
            "lesion_description": lesion_description
        }
        report_str = rag_chain.invoke(rag_input)
        structured_report = json.loads(report_str)
    except Exception as e:
        logger.error(f"[{trace_id}] RAG链处理失败: {e}")
        raise HTTPException(status_code=500, detail=f"生成诊断报告时出错: {e}")
        
    logger.info(f"[{trace_id}] 诊断流程成功完成。")
    return DiagnosisResponse(
        trace_id=trace_id,
        dr_grade=dr_grade,
        dr_grade_desc=dr_grade_desc,
        confidence=confidence,
        lesion_description=lesion_description,
        structured_report=structured_report
    )

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

--- END OF FILE: main.py ---


--- START OF FILE: medical-o1-reasoning-SFT/convert.py ---

import json
import os
import random
from tqdm import tqdm

# --- 配置区 ---
# 获取脚本所在的目录
script_dir = os.path.dirname(os.path.abspath(__file__))

# 将输入文件路径定义为相对于脚本目录的路径
input_file_path = os.path.join(script_dir, 'medical_o1_sft_Chinese.json')
# --------------------

# 定义抽样后的样本数量
SAMPLE_SIZE = 1000
# 定义输出文件路径 (输出文件也将保存在与脚本相同的目录中)
output_file_path = os.path.join(script_dir, f'medical_sharegpt_format_sampled_{SAMPLE_SIZE}.json')


try:
    # 1. 读取原始JSON数据
    print(f"Reading original data from '{input_file_path}'...")
    with open(input_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    total_items = len(data)
    print(f"Successfully loaded {total_items} items.")

    # 2. 对数据集进行随机抽样
    # 如果原始数据量小于等于设定的样本量，则使用全部数据
    if total_items <= SAMPLE_SIZE:
        print(f"Dataset size ({total_items}) is less than or equal to the sample size ({SAMPLE_SIZE}). Using all data.")
        sampled_data = data
    else:
        print(f"Randomly sampling {SAMPLE_SIZE} items from the dataset...")
        sampled_data = random.sample(data, SAMPLE_SIZE)

    # 3. 创建并填充输出结构
    output_data = []
    actual_sample_size = len(sampled_data)
    print(f"Starting conversion for {actual_sample_size} sampled items...")

    for item in tqdm(sampled_data, desc="Processing items"):
        # 使用 .get() 方法确保即使字段缺失也不会报错
        question = item.get("Question", "")
        complex_cot = item.get("Complex_CoT", "")
        response = item.get("Response", "")

        conversation = {
            "conversations": [
                {
                    "from": "human",
                    "value": question
                },
                {
                    "from": "gpt",
                    "value": f"<think>{complex_cot}</think> {response}"
                }
            ],
            "system": "您是一位医学专家，在临床推理、诊断和治疗计划方面拥有丰富的知识。请回答以下医学问题。"
        }
        output_data.append(conversation)

    # 4. 将转换后的数据写入新的JSON文件
    with open(output_file_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=4)

    print(f"\nConversion completed successfully!")
    print(f"{actual_sample_size} sampled items saved to '{output_file_path}'")

except FileNotFoundError:
    print(f"Error: The file '{input_file_path}' was not found. Please check the file path.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

--- END OF FILE: medical-o1-reasoning-SFT/convert.py ---


--- START OF FILE: rag_chain_builder.py ---

# rag_chain_builder.py
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.language_models.chat_models import BaseChatModel

from settings import settings
import logging

logger = logging.getLogger(__name__)

def get_retriever():
    """创建并返回一个知识库检索器。"""
    embeddings = HuggingFaceEmbeddings(model_name=settings.EMBEDDING_MODEL)
    if os.path.exists(settings.VECTOR_DB_PATH):
        vector_store = FAISS.load_local(settings.VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
        logger.info("从本地加载向量数据库。")
    else:
        loader = DirectoryLoader(settings.KNOWLEDGE_BASE_PATH, glob="**/*.txt")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=settings.CHUNK_SIZE, chunk_overlap=settings.CHUNK_OVERLAP)
        docs = text_splitter.split_documents(documents)
        vector_store = FAISS.from_documents(docs, embeddings)
        vector_store.save_local(settings.VECTOR_DB_PATH)
        logger.info("创建并保存了新的向量数据库。")
    return vector_store.as_retriever(search_kwargs={"k": settings.TOP_K})

def create_rag_chain(llm: BaseChatModel, retriever):
    """使用LCEL构建并返回RAG链。"""
    prompt_template = """
    ## 角色定位
    你是一名专业的眼科医生AI助手。

    ## 核心任务
    根据提供的`DR分级结果`、`关键病灶描述`和`参考知识`，生成一份结构化、可解释、可追溯的诊疗决策报告。
    你必须严格遵循思维链（Chain-of-Thought）进行推理。

    ## 参考知识
    {context}

    ## 诊断信息
    - DR分级结果: {dr_grade_desc}
    - 关键病灶描述: {lesion_description}

    ## 推理与决策
    请基于以上信息，进行链式思维(Chain-of-Thought)推理，并生成最终的诊疗决策。
    
    ### 思考过程:
    让我一步步思考：
    1.  **病灶与分级关联分析**: 病灶描述 '{lesion_description}' 与诊断 '{dr_grade_desc}' 是否一致？知识库中对此有何说明？
    2.  **风险评估**: 根据分级和病灶，患者的视力丧失风险有多大？是否有进展迹象？
    3.  **治疗方案选择**: 知识库中针对 '{dr_grade_desc}' 推荐了哪些治疗方案？结合具体病灶，哪种最合适？
    
    ### 结构化输出:
    {format_instructions}
    """
    
    format_instructions = """
    请严格按照以下JSON格式输出，不要包含任何代码块标记或额外说明：
    {
      "cot_reasoning": "（这里是你详细的思考过程）",
      "recommendations": [
        "（具体的治疗建议1）",
        "（具体的随访计划建议2）"
      ],
      "traceability": "（引用支持你决策的`参考知识`中的关键句子）"
    }
    """
    
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "dr_grade_desc", "lesion_description"],
        partial_variables={"format_instructions": format_instructions}
    )
    
    # 定义输入到RAG链的数据结构
    setup = RunnableParallel(
        context=lambda x: retriever.get_relevant_documents(x["lesion_description"]),
        dr_grade_desc=lambda x: x["dr_grade_desc"],
        lesion_description=lambda x: x["lesion_description"]
    )
    
    rag_chain = setup | prompt | llm | StrOutputParser()
    logger.info("LangChain RAG链构建完成。")
    return rag_chain

--- END OF FILE: rag_chain_builder.py ---


--- START OF FILE: run_diagnosis.py ---

# run_diagnosis.py
import requests
import argparse
import json
import time

def run_diagnosis(image_path: str, server_url: str = "http://127.0.0.1:8000/diagnose"):
    """向诊断API发送一张图片并打印结果。"""
    try:
        with open(image_path, "rb") as f:
            files = {"file": (image_path, f, "image/jpeg")}
            print(f"正在发送图片 '{image_path}' 到 {server_url}...")
            
            start_time = time.time()
            response = requests.post(server_url, files=files, timeout=300) # 5分钟超时
            end_time = time.time()
            
            response.raise_for_status()

            print(f"\n诊断成功！耗时: {end_time - start_time:.2f} 秒\n" + "="*80)
            
            result = response.json()
            print(json.dumps(result, indent=2, ensure_ascii=False))
            print("="*80)

    except FileNotFoundError:
        print(f"错误：找不到文件 '{image_path}'")
    except requests.exceptions.RequestException as e:
        print(f"错误：请求失败 - {e}")
        if e.response:
            print("服务器返回内容:", e.response.text)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="一键运行DR图像智能诊断客户端")
    parser.add_argument("image_path", type=str, help="需要诊断的DR图像文件路径")
    args = parser.parse_args()
    
    run_diagnosis(args.image_path)

--- END OF FILE: run_diagnosis.py ---


--- START OF FILE: settings.py ---

# settings.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    """系统配置，可被环境变量或.env文件覆盖"""
    # 模型路径
    RESNET_MODEL_PATH: str = "./models/resnet50_dr_grading.pth"
    QWEN_VL_MODEL_PATH: str = "./models/Qwen-VL"
    R1_7B_MODEL_PATH: str = "./models/R1-7B-finetuned"
    EMBEDDING_MODEL: str = "sentence-transformers/all-MiniLM-L6-v2"
    
    # RAG配置
    VECTOR_DB_PATH: str = "./vector_db"
    KNOWLEDGE_BASE_PATH: str = "./knowledge_base"
    CHUNK_SIZE: int = 500
    CHUNK_OVERLAP: int = 50
    TOP_K: int = 3
    
    # DR分级映射
    DR_GRADES: dict = {
        0: "无DR", 1: "轻度非增殖性DR", 2: "中度非增殖性DR",
        3: "重度非增殖性DR", 4: "增殖性DR"
    }

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

# 创建一个全局配置实例供其他模块使用
settings = Settings()

--- END OF FILE: settings.py ---


--- START OF FILE: vision_processors.py ---

# vision_processors.py
import os
import torch
import torch.nn as nn
from PIL import Image
from transformers import AutoProcessor, AutoModelForCausalLM
import torchvision.models as models
from torchvision import transforms
from typing import Tuple

from settings import settings
import logging

logger = logging.getLogger(__name__)

class DRGradingModule:
    """DR图像分级模块"""
    def __init__(self, model_path: str):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self._load_model(model_path)
        self.transform = self._get_transform()
    
    def _load_model(self, model_path: str) -> nn.Module:
        model = models.resnet50(weights=None)
        num_features = model.fc.in_features
        model.fc = nn.Linear(num_features, len(settings.DR_GRADES))
        if os.path.exists(model_path):
            model.load_state_dict(torch.load(model_path, map_location=self.device))
            logger.info(f"成功加载DR分级模型: {model_path}")
        else:
            logger.warning(f"DR分级模型不存在: {model_path}，使用随机初始化")
        model = model.to(self.device)
        model.eval()
        return model
    
    def _get_transform(self):
        return transforms.Compose([
            transforms.Resize((224, 224)), transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    @torch.no_grad()
    def predict(self, image: Image.Image) -> Tuple[int, float]:
        img_tensor = self.transform(image).unsqueeze(0).to(self.device)
        outputs = self.model(img_tensor)
        probabilities = torch.softmax(outputs, dim=1)
        confidence, predicted = torch.max(probabilities, 1)
        grade = predicted.item()
        conf = confidence.item()
        logger.info(f"DR分级结果: 等级={grade}, 置信度={conf:.3f}")
        return grade, conf

class QwenVLModule:
    """Qwen-VL视觉语言模型模块"""
    def __init__(self, model_path: str):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model, self.processor = self._load_model(model_path)
    
    def _load_model(self, model_path: str):
        try:
            processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True)
            logger.info(f"成功加载Qwen-VL模型: {model_path}")
            return model, processor
        except Exception as e:
            logger.error(f"加载Qwen-VL模型失败: {e}")
            return None, None
    
    def generate_description(self, image: Image.Image, dr_grade_desc: str) -> str:
        if self.model is None: return "Qwen-VL模型未加载"
        prompt = f"这是一张诊断为'{dr_grade_desc}'的眼底图，请用专业术语详细描述图中的关键病灶特征。"
        inputs = self.processor(text=prompt, images=image, return_tensors="pt").to(self.model.device)
        generated_ids = self.model.generate(**inputs, max_new_tokens=256)
        description = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
        final_desc = description.replace(prompt, "").strip()
        logger.info("成功生成病灶描述")
        return final_desc

--- END OF FILE: vision_processors.py ---


